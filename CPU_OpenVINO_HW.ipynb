{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "febba31b-151d-45dc-bffd-c8c881fe287b",
   "metadata": {},
   "source": [
    "# OpenVINO \n",
    "\n",
    "## Установим Зависимости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525d41e2-e0ec-4d0a-8500-ce5542cada38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install -U openvino nncf\n",
    "# # или пре-релизная версия:\n",
    "#!pip install --pre -U openvino --extra-index-url https://storage.openvinotoolkit.org/simple/wheels/nightly git+https://github.com/openvinotoolkit/nncf.git\n",
    "\n",
    "#!pip install transformers[torch] datasets evaluate ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78e47b0-17c4-433a-9862-dbcc341fbe36",
   "metadata": {},
   "source": [
    "## Скачиваем Предобученную Модель\n",
    "\n",
    "Берем классификационную модель из [Huggingface Hub](https://huggingface.co/models?pipeline_tag=text-classification&sort=trending&search=sst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fe5515-d107-4218-809e-f2dbde0c29aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_id = \"philschmid/MiniLM-L6-H384-uncased-sst2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "hf_model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "\n",
    "hf_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085ca21d",
   "metadata": {},
   "source": [
    "Сохраним оригинальные веса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19d840b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(hf_model.state_dict(), \"model_sst.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccafc42d-44e3-4551-928b-b35f72f5739a",
   "metadata": {},
   "source": [
    "## Конвертируем Модель в OpenVINO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a04e9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7])\n",
      "torch.Size([1, 7])\n"
     ]
    }
   ],
   "source": [
    "import openvino as ov\n",
    "\n",
    "hf_model.eval()\n",
    "inputs = {**tokenizer(\"This lesson was amazing!\", \n",
    "                      return_tensors=\"pt\")}\n",
    "ov_model = ov.convert_model(hf_model, example_input=v)\n",
    "ov.save_model(ov_model, 'model.xml')\n",
    "\n",
    "compiled_model = ov.compile_model(ov_model)\n",
    "print(compiled_model(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8180f9de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2023, 10800,  2001,  6429,   999,   102]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75430162",
   "metadata": {},
   "source": [
    "Конвертируем модель с простым входным тензором"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dad7b85-8ec6-4ba6-9cd7-18c859df16aa",
   "metadata": {},
   "source": [
    "Провалидируем предсказания сконвертированной модели:\n",
    "\n",
    "- PyTorch accuracy:  **0.90138**\n",
    "- OpenVINO accuracy: **0.90138**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bdbcbd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3fb45710-d3f2-41bc-8d67-269b8b12abf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch accuracy:  0.9013761467889908\n",
      "OpenVINO accuracy: 0.9013761467889908\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "\n",
    "\n",
    "val_dataset = load_dataset(\"glue\", \"sst2\", split=\"validation\")\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "@torch.no_grad\n",
    "def accuracy_evaluate(model, dataset=val_dataset, accuracy=accuracy):   \n",
    "    for sample in dataset:\n",
    "        tokenized = {**tokenizer(sample[\"sentence\"], return_tensors=\"pt\")}\n",
    "        logits = model(tokenized)[\"logits\"]\n",
    "        pred = np.argmax(logits, axis=1)\n",
    "        accuracy.add(references=sample[\"label\"], predictions=pred)\n",
    "\n",
    "    return accuracy.compute()[\"accuracy\"]\n",
    "\n",
    "\n",
    "print(f\"PyTorch accuracy:  {accuracy_evaluate(lambda x: hf_model(**x))}\")\n",
    "print(f\"OpenVINO accuracy: {accuracy_evaluate(compiled_model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1215c7a2-4011-4f61-94e8-cb48cc8c4312",
   "metadata": {},
   "source": [
    "## Benchmark\n",
    "\n",
    "Добавим несколько инференсов в бенчмарк для того, чтобы получить более точные результаты."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e13ead",
   "metadata": {},
   "source": [
    "* Pytorch:   7.33648s, FPS=118.858, latency: 0.00647s, 0.00830s, 0.01716s\n",
    "* Openvino:  5.27496s, FPS=165.309, latency: 0.00343s, 0.00606s, 0.01561s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378d80b6-9988-4dc1-927a-a2d1a5bfa382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch:   20.02967s, FPS=43.535, latency: 0.00604s, 0.00750s, 0.01791s\n",
      "Openvino:  15.10048s, FPS=57.747, latency: 0.00312s, 0.00576s, 0.01241s\n"
     ]
    }
   ],
   "source": [
    "from time import perf_counter\n",
    "from statistics import median\n",
    "\n",
    "\n",
    "@torch.no_grad\n",
    "def benchmark(model, dataset):\n",
    "    tokenized_dataset = [{**tokenizer(sample[\"sentence\"], return_tensors=\"pt\")} for sample in dataset]\n",
    "\n",
    "    # warmup\n",
    "    for data in tokenized_dataset[:10]:\n",
    "        model(data)\n",
    "        \n",
    "    times = []\n",
    "    for _ in range(3):\n",
    "        for data in tokenized_dataset:\n",
    "            start = perf_counter()\n",
    "            model(data)\n",
    "            end = perf_counter()\n",
    "            times.append(end - start)\n",
    "\n",
    "    return (\n",
    "        f\"{sum(times):.5f}s, FPS={(len(dataset) / sum(times)):.3f}, \"\n",
    "        f\"latency: {min(times):.5f}s, {median(times):.5f}s, {max(times):.5f}s\"\n",
    "    )\n",
    "\n",
    "print(\"Pytorch:  \", benchmark(lambda x: hf_model(**x), val_dataset))\n",
    "print(\"Openvino: \", benchmark(lambda x: compiled_model(x), val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f9cec1-309b-457d-b388-d4bdf0792762",
   "metadata": {},
   "source": [
    "## Inference Hints\n",
    "\n",
    "Скомпилируем модель с разными инференс хинтами и сравним результаты бенчмарка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8303d294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CPU', 'GPU']\n",
      "['FP32', 'INT8', 'BIN', 'EXPORT_IMPORT']\n"
     ]
    }
   ],
   "source": [
    "import openvino.properties as props\n",
    "import openvino.properties.hint as hints\n",
    "\n",
    "core = ov.Core()\n",
    "print(core.available_devices)\n",
    "print(core.get_property('CPU', props.device.capabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe2401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_througput = ov.compile_model(\n",
    "    ov_model, \n",
    "    \"CPU\", \n",
    "    {hints.performance_mode: hints.PerformanceMode.THROUGHPUT}\n",
    ")\n",
    "print(\"Openvino: \", benchmark(lambda x: compiled_througput(x), val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a653f8e-6b4a-4727-ad67-7414973aee60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Openvino:  10.86417s, FPS=80.264, latency: 0.00192s, 0.00393s, 0.01735s\n"
     ]
    }
   ],
   "source": [
    "ov_model = ov.convert_model(hf_model, example_input= dict(inputs))\n",
    "\n",
    "compiled_througput = ov.compile_model(\n",
    "    ov_model, \n",
    "    \"CPU\", \n",
    "    {hints.performance_mode: hints.PerformanceMode.THROUGHPUT}\n",
    ")\n",
    "print(\"Openvino: \", benchmark(lambda x: compiled_througput(x), val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cee26a4-edb8-47cc-a533-659cba5e4418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Openvino:  11.55482s, FPS=75.466, latency: 0.00233s, 0.00439s, 0.00849s\n"
     ]
    }
   ],
   "source": [
    "compiled_latency = ov.compile_model(\n",
    "    ov_model, \n",
    "    \"CPU\", \n",
    "    {hints.performance_mode: hints.PerformanceMode.LATENCY}\n",
    ")\n",
    "print(\"Openvino: \", benchmark(lambda x: compiled_latency(x), val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9caf3b-059c-45b7-a036-fb29ef3fd1b8",
   "metadata": {},
   "source": [
    "## Async Inference\n",
    "\n",
    "Перепишем  бенчмарк под асинхронный инференс. Он должен принимать на вход асинхронную очередь и датасет.\n",
    "\n",
    "### Простой Бенчмарк\n",
    "Простая версия бенчмарка должна замерить FPS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7762d37-ee4f-4e4b-8df5-1974c934e79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Openvino:  1.98564s, FPS=439.152\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Any, Optional\n",
    "\n",
    "\n",
    "def completion_callback(\n",
    "    infer_request: ov.InferRequest, user_data: Optional[Dict[str, Any]] = None\n",
    ") -> None:\n",
    "    ...\n",
    "\n",
    "infer_queue = ov.AsyncInferQueue(compiled_througput)\n",
    "infer_queue.set_callback(completion_callback)\n",
    "\n",
    "def simple_benchmark_async(queue, dataset):\n",
    "    tokenized_dataset = [{**tokenizer(sample[\"sentence\"], return_tensors=\"np\")} for sample in dataset]\n",
    "\n",
    "    # warmup\n",
    "    for data in tokenized_dataset[:10]:\n",
    "        queue.start_async(data)\n",
    "    queue.wait_all()\n",
    "\n",
    "    elapsed_start = perf_counter()\n",
    "    for idx, data in enumerate(tokenized_dataset):\n",
    "        queue.start_async(data) \n",
    "    queue.wait_all()\n",
    "    elapsed_end = perf_counter()\n",
    "    elapsed = elapsed_end - elapsed_start\n",
    "\n",
    "    return f\"{elapsed:.5f}s, FPS={(len(dataset) / elapsed):.3f}\"\n",
    "\n",
    "print(\"Openvino: \", simple_benchmark_async(infer_queue, val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58f675e-f984-4ab1-94dc-6023e3b3cc58",
   "metadata": {},
   "source": [
    "### Добавим Измерение latency В Асинхронный Бенчмарк\n",
    "\n",
    "Используем `completion_callback` для подсчёта latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0808d06-8d27-457f-af01-2e62bc8252ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Openvino:  2.00049s, FPS=435.893, latency: 0.00000s, 0.00143s, 0.01041s\n"
     ]
    }
   ],
   "source": [
    "def completion_callback(\n",
    "    infer_request: ov.InferRequest,\n",
    "    user_data: Dict[str, Any],\n",
    ") -> None:\n",
    "    end = perf_counter()  # инференс завершился, заменяем время\n",
    "    idx = user_data[\"idx\"]\n",
    "    times = user_data[\"times\"]\n",
    "    times[idx] = end - times[idx]  # вычитаем время начала \n",
    "\n",
    "\n",
    "# используем существующую очередь, переназначим коллбэк\n",
    "infer_queue.set_callback(completion_callback)\n",
    "\n",
    "\n",
    "def benchmark_async(queue, dataset):\n",
    "    tokenized_dataset = [{**tokenizer(sample[\"sentence\"], return_tensors=\"np\")} for sample in dataset]\n",
    "    times = [0 for _ in range(len(dataset))]\n",
    "\n",
    "    # warmup\n",
    "    for data in tokenized_dataset[:10]:\n",
    "        queue.start_async(data, {\"idx\": 0, \"times\": times})\n",
    "    queue.wait_all()\n",
    "    \n",
    "    start = perf_counter()\n",
    "    for idx, data in enumerate(tokenized_dataset):\n",
    "        # записываем время старта реквеста в массив по индексу входных данных\n",
    "        times[idx] = perf_counter()\n",
    "        # передаём индекс и массив с началами вместе с входными данными\n",
    "        queue.start_async(data, {\"idx\": idx, \"times\": times})\n",
    "    # ждём пока завершатся все реквесты\n",
    "    queue.wait_all()\n",
    "\n",
    "    # замеряем время конца инференса\n",
    "    end = perf_counter()\n",
    "    # для общего времени исполнения уже нельзя брать sum(times), так как реквесты исполняются одновременно\n",
    "    elapsed = end - start\n",
    "    \n",
    "    return (\n",
    "        f\"{elapsed:.5f}s, FPS={(len(dataset) / elapsed):.3f}, \"\n",
    "        f\"latency: {min(times):.5f}s, {median(times):.5f}s, {max(times):.5f}s\"\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Openvino: \", benchmark_async(infer_queue, val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688d6b8a",
   "metadata": {},
   "source": [
    "`InferRequest` объект сам замеряет latency во время инференса, поэтому можно просто достать время оттуда. Время замеряется в плюсах, поэтому latency получается немного меньше. benchmark app для замеров latency тоже берёт информацию из реквеста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9210fe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latency_from_ir_completion_callback(\n",
    "    infer_request: ov.InferRequest,\n",
    "    user_data: Dict[str, Any],\n",
    ") -> None:\n",
    "    times = user_data[\"times\"]\n",
    "    idx = user_data[\"idx\"]\n",
    "    times[idx] = infer_request.latency * 1e-3  # ms -> s\n",
    "\n",
    "\n",
    "infer_queue.set_callback(latency_from_ir_completion_callback)\n",
    "print(\"Openvino: \", benchmark_async(infer_queue, val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b9485b",
   "metadata": {},
   "source": [
    "### Async Accuracy Evaluation\n",
    "\n",
    "Напишим функцию, которая измеряет `accuracy` в режиме асинхронного инференса. Для получения доступа к данным из `InferRequest` можно использовать метод `infer_request.get_tensor(\"<output_name>\").data`. Синхронная функция для референса:\n",
    "```python\n",
    "def accuracy_evaluate(model, dataset=val_dataset, accuracy=accuracy):   \n",
    "    for sample in dataset:\n",
    "        tokenized = {**tokenizer(sample[\"sentence\"], return_tensors=\"pt\")}\n",
    "        logits = model(tokenized)[\"logits\"]\n",
    "        pred = np.argmax(logits, axis=1)\n",
    "        accuracy.add(references=sample[\"label\"], predictions=pred)\n",
    "\n",
    "    return accuracy.compute()[\"accuracy\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38e9cd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 872/872 [00:02<00:00, 434.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.9013761467889908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "def completion_callback(\n",
    "    infer_request: ov.InferRequest,\n",
    "    user_data: Dict[str, Any],\n",
    ") -> None:\n",
    "    logits = infer_request.get_tensor(\"logits\").data\n",
    "    pred = np.argmax(logits, axis=1)\n",
    "\n",
    "    predictions.append(pred.item())\n",
    "    references.append(user_data[\"label\"])\n",
    "\n",
    "\n",
    "# используем существующую очередь, переназначим коллбэк\n",
    "infer_queue.set_callback(completion_callback)\n",
    "\n",
    "\n",
    "def async_accuracy_evaluate(queue, dataset=val_dataset, accuracy=accuracy):\n",
    "\n",
    "    \n",
    "    for sample in tqdm(dataset):\n",
    "        tokenized = {**tokenizer(sample[\"sentence\"], return_tensors=\"np\")}\n",
    "        user_data = {\"label\": sample[\"label\"]}\n",
    "        queue.start_async(tokenized, user_data) \n",
    "\n",
    "\n",
    "    queue.wait_all()\n",
    "    accuracy.add_batch(references=references, predictions=predictions)\n",
    "    return accuracy.compute()[\"accuracy\"]\n",
    "\n",
    "\n",
    "print(f\"Model accuracy: {async_accuracy_evaluate(infer_queue)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223d2dc7-572e-4c17-b7f0-e7efdff8f0bc",
   "metadata": {},
   "source": [
    "## Benchmark App\n",
    "\n",
    "### Измерим Производительность Модели с Помощью CLI benchmark_app\n",
    "\n",
    "Чтобы не ждать по минуте используем флаг `-t 30`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0092a0-5b06-42b5-aa3c-0571ad6d3aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ INFO ] Parsing input parameters\n",
      "[Step 2/11] Loading OpenVINO Runtime\n",
      "[ INFO ] OpenVINO:\n",
      "[ INFO ] Build ................................. 2025.0.0-17942-1f68be9f594-releases/2025/0\n",
      "[ INFO ] \n",
      "[ INFO ] Device info:\n",
      "[ INFO ] CPU\n",
      "[ INFO ] Build ................................. 2025.0.0-17942-1f68be9f594-releases/2025/0\n",
      "[ INFO ] \n",
      "[ INFO ] \n",
      "[Step 3/11] Setting device configuration\n",
      "[ WARNING ] Performance hint was not explicitly specified in command line. Device(CPU) performance hint will be set to PerformanceMode.THROUGHPUT.\n",
      "[Step 4/11] Reading model files\n",
      "[ INFO ] Loading model files\n",
      "[ INFO ] Read model took 43.67 ms\n",
      "[ INFO ] Original model I/O parameters:\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input_ids (node: input_ids) : i64 / [...] / [?,?]\n",
      "[ INFO ]     attention_mask , 63 (node: attention_mask) : i64 / [...] / [?,?]\n",
      "[ INFO ]     token_type_ids (node: token_type_ids) : i64 / [...] / [?,?]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     logits (node: __module.classifier/aten::linear/Add) : f32 / [...] / [?,2]\n",
      "[Step 5/11] Resizing model to match image sizes and given batch\n",
      "[ INFO ] Model batch size: 1\n",
      "[ INFO ] Reshaping model: 'input_ids': [1,128], '63': [1,128], 'token_type_ids': [1,128]\n",
      "[ INFO ] Reshape model took 10.78 ms\n",
      "[Step 6/11] Configuring input of the model\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input_ids (node: input_ids) : i64 / [...] / [1,128]\n",
      "[ INFO ]     attention_mask , 63 (node: attention_mask) : i64 / [...] / [1,128]\n",
      "[ INFO ]     token_type_ids (node: token_type_ids) : i64 / [...] / [1,128]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     logits (node: __module.classifier/aten::linear/Add) : f32 / [...] / [1,2]\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Compile model took 821.49 ms\n",
      "[Step 8/11] Querying optimal runtime parameters\n",
      "[ INFO ] Model:\n",
      "[ INFO ]   NETWORK_NAME: Model195\n",
      "[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 12\n",
      "[ INFO ]   NUM_STREAMS: 12\n",
      "[ INFO ]   INFERENCE_NUM_THREADS: 16\n",
      "[ INFO ]   PERF_COUNT: NO\n",
      "[ INFO ]   INFERENCE_PRECISION_HINT: <Type: 'float32'>\n",
      "[ INFO ]   PERFORMANCE_HINT: THROUGHPUT\n",
      "[ INFO ]   EXECUTION_MODE_HINT: ExecutionMode.PERFORMANCE\n",
      "[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS: 0\n",
      "[ INFO ]   ENABLE_CPU_PINNING: False\n",
      "[ INFO ]   SCHEDULING_CORE_TYPE: SchedulingCoreType.ANY_CORE\n",
      "[ INFO ]   MODEL_DISTRIBUTION_POLICY: set()\n",
      "[ INFO ]   ENABLE_HYPER_THREADING: True\n",
      "[ INFO ]   EXECUTION_DEVICES: ['CPU']\n",
      "[ INFO ]   CPU_DENORMALS_OPTIMIZATION: False\n",
      "[ INFO ]   LOG_LEVEL: Level.NO\n",
      "[ INFO ]   CPU_SPARSE_WEIGHTS_DECOMPRESSION_RATE: 1.0\n",
      "[ INFO ]   DYNAMIC_QUANTIZATION_GROUP_SIZE: 32\n",
      "[ INFO ]   KV_CACHE_PRECISION: <Type: 'uint8_t'>\n",
      "[ INFO ]   KEY_CACHE_PRECISION: <Type: 'uint8_t'>\n",
      "[ INFO ]   VALUE_CACHE_PRECISION: <Type: 'uint8_t'>\n",
      "[ INFO ]   KEY_CACHE_GROUP_SIZE: 0\n",
      "[ INFO ]   VALUE_CACHE_GROUP_SIZE: 0\n",
      "[Step 9/11] Creating infer requests and preparing input tensors\n",
      "[ WARNING ] No input files were given for input 'input_ids'!. This input will be filled with random values!\n",
      "[ WARNING ] No input files were given for input '63'!. This input will be filled with random values!\n",
      "[ WARNING ] No input files were given for input 'token_type_ids'!. This input will be filled with random values!\n",
      "[ INFO ] Fill input 'input_ids' with random values \n",
      "[ INFO ] Fill input '63' with random values \n",
      "[ INFO ] Fill input 'token_type_ids' with random values \n",
      "[Step 10/11] Measuring performance (Start inference asynchronously, 12 inference requests, limits: 30000 ms duration)\n",
      "[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).\n",
      "[ INFO ] First inference took 78.89 ms\n",
      "[Step 11/11] Dumping statistics report\n",
      "[ INFO ] Execution Devices:['CPU']\n",
      "[ INFO ] Count:            3060 iterations\n",
      "[ INFO ] Duration:         30116.34 ms\n",
      "[ INFO ] Latency:\n",
      "[ INFO ]    Median:        93.08 ms\n",
      "[ INFO ]    Average:       117.87 ms\n",
      "[ INFO ]    Min:           48.33 ms\n",
      "[ INFO ]    Max:           1508.28 ms\n",
      "[ INFO ] Throughput:   101.61 FPS\n"
     ]
    }
   ],
   "source": [
    "!benchmark_app -m \"bert.xml\" -shape [1,128] -t 30 | tail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6bb3fa-1814-4bfe-bb6b-23b508ee8c69",
   "metadata": {},
   "source": [
    "###  Попробуем подобрать параметры, чтобы увеличить FPS относительно\n",
    "\n",
    "101 FPS -> 156 FPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f04ae9-1461-4fa6-a512-8e5eb1e91214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ INFO ] Parsing input parameters\n",
      "[Step 2/11] Loading OpenVINO Runtime\n",
      "[ INFO ] OpenVINO:\n",
      "[ INFO ] Build ................................. 2025.0.0-17942-1f68be9f594-releases/2025/0\n",
      "[ INFO ] \n",
      "[ INFO ] Device info:\n",
      "[ INFO ] CPU\n",
      "[ INFO ] Build ................................. 2025.0.0-17942-1f68be9f594-releases/2025/0\n",
      "[ INFO ] \n",
      "[ INFO ] \n",
      "[Step 3/11] Setting device configuration\n",
      "[Step 4/11] Reading model files\n",
      "[ INFO ] Loading model files\n",
      "[ INFO ] Read model took 18.68 ms\n",
      "[ INFO ] Original model I/O parameters:\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input_ids (node: input_ids) : i64 / [...] / [?,?]\n",
      "[ INFO ]     attention_mask , 63 (node: attention_mask) : i64 / [...] / [?,?]\n",
      "[ INFO ]     token_type_ids (node: token_type_ids) : i64 / [...] / [?,?]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     logits (node: __module.classifier/aten::linear/Add) : f32 / [...] / [?,2]\n",
      "[Step 5/11] Resizing model to match image sizes and given batch\n",
      "[ INFO ] Model batch size: 1\n",
      "[ INFO ] Reshaping model: 'input_ids': [1,128], '63': [1,128], 'token_type_ids': [1,128]\n",
      "[ INFO ] Reshape model took 0.99 ms\n",
      "[Step 6/11] Configuring input of the model\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input_ids (node: input_ids) : i64 / [...] / [1,128]\n",
      "[ INFO ]     attention_mask , 63 (node: attention_mask) : i64 / [...] / [1,128]\n",
      "[ INFO ]     token_type_ids (node: token_type_ids) : i64 / [...] / [1,128]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     logits (node: __module.classifier/aten::linear/Add) : f32 / [...] / [1,2]\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Compile model took 229.64 ms\n",
      "[Step 8/11] Querying optimal runtime parameters\n",
      "[ INFO ] Model:\n",
      "[ INFO ]   NETWORK_NAME: Model195\n",
      "[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 12\n",
      "[ INFO ]   NUM_STREAMS: 12\n",
      "[ INFO ]   INFERENCE_NUM_THREADS: 16\n",
      "[ INFO ]   PERF_COUNT: NO\n",
      "[ INFO ]   INFERENCE_PRECISION_HINT: <Type: 'float32'>\n",
      "[ INFO ]   PERFORMANCE_HINT: THROUGHPUT\n",
      "[ INFO ]   EXECUTION_MODE_HINT: ExecutionMode.PERFORMANCE\n",
      "[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS: 0\n",
      "[ INFO ]   ENABLE_CPU_PINNING: False\n",
      "[ INFO ]   SCHEDULING_CORE_TYPE: SchedulingCoreType.ANY_CORE\n",
      "[ INFO ]   MODEL_DISTRIBUTION_POLICY: set()\n",
      "[ INFO ]   ENABLE_HYPER_THREADING: True\n",
      "[ INFO ]   EXECUTION_DEVICES: ['CPU']\n",
      "[ INFO ]   CPU_DENORMALS_OPTIMIZATION: False\n",
      "[ INFO ]   LOG_LEVEL: Level.NO\n",
      "[ INFO ]   CPU_SPARSE_WEIGHTS_DECOMPRESSION_RATE: 1.0\n",
      "[ INFO ]   DYNAMIC_QUANTIZATION_GROUP_SIZE: 32\n",
      "[ INFO ]   KV_CACHE_PRECISION: <Type: 'uint8_t'>\n",
      "[ INFO ]   KEY_CACHE_PRECISION: <Type: 'uint8_t'>\n",
      "[ INFO ]   VALUE_CACHE_PRECISION: <Type: 'uint8_t'>\n",
      "[ INFO ]   KEY_CACHE_GROUP_SIZE: 0\n",
      "[ INFO ]   VALUE_CACHE_GROUP_SIZE: 0\n",
      "[Step 9/11] Creating infer requests and preparing input tensors\n",
      "[ WARNING ] No input files were given for input 'input_ids'!. This input will be filled with random values!\n",
      "[ WARNING ] No input files were given for input '63'!. This input will be filled with random values!\n",
      "[ WARNING ] No input files were given for input 'token_type_ids'!. This input will be filled with random values!\n",
      "[ INFO ] Fill input 'input_ids' with random values \n",
      "[ INFO ] Fill input '63' with random values \n",
      "[ INFO ] Fill input 'token_type_ids' with random values \n",
      "[Step 10/11] Measuring performance (Start inference asynchronously, 12 inference requests, limits: 30000 ms duration)\n",
      "[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).\n",
      "[ INFO ] First inference took 28.53 ms\n",
      "[Step 11/11] Dumping statistics report\n",
      "[ INFO ] Execution Devices:['CPU']\n",
      "[ INFO ] Count:            4716 iterations\n",
      "[ INFO ] Duration:         30121.92 ms\n",
      "[ INFO ] Latency:\n",
      "[ INFO ]    Median:        74.37 ms\n",
      "[ INFO ]    Average:       76.45 ms\n",
      "[ INFO ]    Min:           55.72 ms\n",
      "[ INFO ]    Max:           215.39 ms\n",
      "[ INFO ] Throughput:   156.56 FPS\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p benchmark_report\n",
    "!benchmark_app -hint throughput -m model.xml -t 30 -shape [1,128] -t 30 -report_folder benchmark_report -pc -pcsort simple_sort \\\n",
    "-report_type average_counters | tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bfddf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 3 benchmark_report/benchmark_sorted_report.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61649fb9",
   "metadata": {},
   "source": [
    "> ⚠️ Окончательный замер всегда нужно проводить с отключенными perf_counter'ами, так как сбор такой статистики замедляет инференс\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99432787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open(\"ov_config.json\", \"w\") as config_file:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"CPU\": {\n",
    "                \"NUM_STREAMS\": 24,\n",
    "                \"INFERENCE_NUM_THREADS\": 48,\n",
    "            }\n",
    "        },\n",
    "        config_file, \n",
    "    )\n",
    "\n",
    "!benchmark_app -m \"bert.xml\" -shape [1,128] -d CPU -load_config ov_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7505846",
   "metadata": {},
   "source": [
    "За счёт ухудшения latency удалось немного повысить throughput.\n",
    "\n",
    "> ⚠️ Такие измерения нужно производить непосредственно на железе, которое будет использоваться для инференса "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e3523e-1299-4881-a66d-ec2febd54a84",
   "metadata": {},
   "source": [
    "## NNCF\n",
    "\n",
    "### Дефолтная Квантизация\n",
    "\n",
    "Квантизуем модель с дефолтными параметрами. Замерим accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b2aeb8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Openvino:  11.09594s, FPS=78.587, latency: 0.00203s, 0.00402s, 0.01728s\n"
     ]
    }
   ],
   "source": [
    "ov_model = ov.convert_model(hf_model, example_input= dict(inputs))\n",
    "\n",
    "config = {hints.performance_mode: hints.PerformanceMode.THROUGHPUT}\n",
    "compiled_througput = ov.compile_model(ov_model, \"CPU\", config)\n",
    "print(\"Openvino: \", benchmark(lambda x: compiled_througput(x), val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fe28bd-7637-4abc-905e-62bcbe2d4ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nncf\n",
    "\n",
    "\n",
    "def transform_fn(text):\n",
    "    return {**tokenizer(text[\"sentence\"], return_tensors=\"np\")}\n",
    "\n",
    "\n",
    "# возьмём для калибрации другой датасет\n",
    "test_dataset = load_dataset(\"glue\", \"sst2\", split=\"test[:300]\")\n",
    "calibration_dataset = nncf.Dataset(test_dataset, transform_fn)\n",
    "quntized_model = nncf.quantize(\n",
    "    ov_model, \n",
    "    calibration_dataset=calibration_dataset,\n",
    "    preset=nncf.QuantizationPreset.MIXED,\n",
    "    target_device=nncf.TargetDevice.CPU,  # важно\n",
    "    model_type=nncf.ModelType.TRANSFORMER,  # очень важно!\n",
    ")\n",
    "ov.save_model(quntized_model, \"qbert.xml\", compress_to_fp16=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4eb14279-03c3-45a9-8898-f8dd8246343c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenVINO accuracy: 0.8474770642201835\n"
     ]
    }
   ],
   "source": [
    "compiled_quantized = ov.compile_model(quntized_model)\n",
    "print(f\"OpenVINO accuracy: {accuracy_evaluate(compiled_quantized)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48201eff",
   "metadata": {},
   "source": [
    "Против оригинальной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "31b5534b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenVINO accuracy: 0.9013761467889908\n"
     ]
    }
   ],
   "source": [
    "print(f\"OpenVINO accuracy: {accuracy_evaluate(compiled_througput)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0390b0ad-89d2-4d1a-bf8e-eaec372ce89c",
   "metadata": {},
   "source": [
    "Замерим FPS квантизованной модели с помощью benchmark функции или benchmark_app:\n",
    "\n",
    "156 FPS -> 401 FPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0e7741-ff55-49ec-9715-700727e01f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ INFO ] Parsing input parameters\n",
      "[Step 2/11] Loading OpenVINO Runtime\n",
      "[ INFO ] OpenVINO:\n",
      "[ INFO ] Build ................................. 2025.0.0-17942-1f68be9f594-releases/2025/0\n",
      "[ INFO ] \n",
      "[ INFO ] Device info:\n",
      "[ INFO ] CPU\n",
      "[ INFO ] Build ................................. 2025.0.0-17942-1f68be9f594-releases/2025/0\n",
      "[ INFO ] \n",
      "[ INFO ] \n",
      "[Step 3/11] Setting device configuration\n",
      "[Step 4/11] Reading model files\n",
      "[ INFO ] Loading model files\n",
      "[ INFO ] Read model took 18.73 ms\n",
      "[ INFO ] Original model I/O parameters:\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input_ids (node: input_ids) : i64 / [...] / [?,?]\n",
      "[ INFO ]     attention_mask , 63 (node: attention_mask) : i64 / [...] / [?,?]\n",
      "[ INFO ]     token_type_ids (node: token_type_ids) : i64 / [...] / [?,?]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     logits (node: __module.classifier/aten::linear/Add) : f32 / [...] / [?,2]\n",
      "[Step 5/11] Resizing model to match image sizes and given batch\n",
      "[ INFO ] Model batch size: 1\n",
      "[ INFO ] Reshaping model: 'input_ids': [1,128], '63': [1,128], 'token_type_ids': [1,128]\n",
      "[ INFO ] Reshape model took 2.00 ms\n",
      "[Step 6/11] Configuring input of the model\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input_ids (node: input_ids) : i64 / [...] / [1,128]\n",
      "[ INFO ]     attention_mask , 63 (node: attention_mask) : i64 / [...] / [1,128]\n",
      "[ INFO ]     token_type_ids (node: token_type_ids) : i64 / [...] / [1,128]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     logits (node: __module.classifier/aten::linear/Add) : f32 / [...] / [1,2]\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Compile model took 424.93 ms\n",
      "[Step 8/11] Querying optimal runtime parameters\n",
      "[ INFO ] Model:\n",
      "[ INFO ]   NETWORK_NAME: Model54\n",
      "[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 12\n",
      "[ INFO ]   NUM_STREAMS: 12\n",
      "[ INFO ]   INFERENCE_NUM_THREADS: 16\n",
      "[ INFO ]   PERF_COUNT: NO\n",
      "[ INFO ]   INFERENCE_PRECISION_HINT: <Type: 'float32'>\n",
      "[ INFO ]   PERFORMANCE_HINT: THROUGHPUT\n",
      "[ INFO ]   EXECUTION_MODE_HINT: ExecutionMode.PERFORMANCE\n",
      "[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS: 0\n",
      "[ INFO ]   ENABLE_CPU_PINNING: False\n",
      "[ INFO ]   SCHEDULING_CORE_TYPE: SchedulingCoreType.ANY_CORE\n",
      "[ INFO ]   MODEL_DISTRIBUTION_POLICY: set()\n",
      "[ INFO ]   ENABLE_HYPER_THREADING: True\n",
      "[ INFO ]   EXECUTION_DEVICES: ['CPU']\n",
      "[ INFO ]   CPU_DENORMALS_OPTIMIZATION: False\n",
      "[ INFO ]   LOG_LEVEL: Level.NO\n",
      "[ INFO ]   CPU_SPARSE_WEIGHTS_DECOMPRESSION_RATE: 1.0\n",
      "[ INFO ]   DYNAMIC_QUANTIZATION_GROUP_SIZE: 32\n",
      "[ INFO ]   KV_CACHE_PRECISION: <Type: 'uint8_t'>\n",
      "[ INFO ]   KEY_CACHE_PRECISION: <Type: 'uint8_t'>\n",
      "[ INFO ]   VALUE_CACHE_PRECISION: <Type: 'uint8_t'>\n",
      "[ INFO ]   KEY_CACHE_GROUP_SIZE: 0\n",
      "[ INFO ]   VALUE_CACHE_GROUP_SIZE: 0\n",
      "[Step 9/11] Creating infer requests and preparing input tensors\n",
      "[ WARNING ] No input files were given for input 'input_ids'!. This input will be filled with random values!\n",
      "[ WARNING ] No input files were given for input '63'!. This input will be filled with random values!\n",
      "[ WARNING ] No input files were given for input 'token_type_ids'!. This input will be filled with random values!\n",
      "[ INFO ] Fill input 'input_ids' with random values \n",
      "[ INFO ] Fill input '63' with random values \n",
      "[ INFO ] Fill input 'token_type_ids' with random values \n",
      "[Step 10/11] Measuring performance (Start inference asynchronously, 12 inference requests, limits: 30000 ms duration)\n",
      "[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).\n",
      "[ INFO ] First inference took 13.28 ms\n",
      "[Step 11/11] Dumping statistics report\n",
      "[ INFO ] Execution Devices:['CPU']\n",
      "[ INFO ] Count:            12060 iterations\n",
      "[ INFO ] Duration:         30026.83 ms\n",
      "[ INFO ] Latency:\n",
      "[ INFO ]    Median:        28.06 ms\n",
      "[ INFO ]    Average:       29.77 ms\n",
      "[ INFO ]    Min:           17.47 ms\n",
      "[ INFO ]    Max:           125.07 ms\n",
      "[ INFO ] Throughput:   401.64 FPS\n"
     ]
    }
   ],
   "source": [
    "!benchmark_app -m \"qbert.xml\" -shape [1,128] -t 30 | tail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829e956c-bf5b-43b6-94e5-120dd582609e",
   "metadata": {},
   "source": [
    "### Accuracy Control\n",
    "\n",
    "Квантизуем модель так, чтобы потеря accuracy была в пределах 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc946fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# разделим валидационный датасет для финальной валидации\n",
    "validation_dataset = list(val_dataset)\n",
    "final_test_dataset, validation_dataset = validation_dataset[:-300], validation_dataset[-300:]\n",
    "validation_dataset = nncf.Dataset(validation_dataset, transform_fn)\n",
    "\n",
    "quntized_model = nncf.quantize_with_accuracy_control(\n",
    "    model=ov_model,\n",
    "    calibration_dataset=calibration_dataset,\n",
    "    preset=nncf.QuantizationPreset.PERFORMANCE,\n",
    "    validation_dataset=validation_dataset,\n",
    "    validation_fn=accuracy_evaluate,  # функция замера accuracy переиспользуется\n",
    "    max_drop=0.01,\n",
    "    target_device=nncf.TargetDevice.CPU,\n",
    "    drop_type=nncf.DropType.ABSOLUTE,\n",
    "    # уберём тип модели, чтобы получить accuracy drop больше 1%\n",
    "    # иначе квантизация будет совпадать с дефолтной 🤷\n",
    "    # model_type=nncf.ModelType.TRANSFORMER, \n",
    ")\n",
    "ov.save_model(quntized_model, \"qbert_acc.xml\", compress_to_fp16=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3aaeac-a8ea-4a3c-948a-4be72524c653",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_quantized_acc = ov.compile_model(\n",
    "    \"qbert_acc.xml\", \"CPU\", {hints.performance_mode: hints.PerformanceMode.THROUGHPUT}\n",
    ")\n",
    "\n",
    "print(f\"Openvino: {accuracy_evaluate(compiled_througput, final_test_dataset)}\")\n",
    "print(f\"Quantized Openvino :  {accuracy_evaluate(compiled_quantized, final_test_dataset)}\")\n",
    "print(f\"Openvino quantized with acc:  {accuracy_evaluate(compiled_quantized_acc, final_test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15296d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino_tokenizers import convert_tokenizer, connect_models\n",
    "\n",
    "\n",
    "def get_connected_model(hf_model, tokenizer):\n",
    "    example_input = {**tokenizer(\"test\", return_tensors=\"pt\")}\n",
    "    ov_model = ov.convert_model(hf_model, example_input=example_input)\n",
    "    ov_tokenizer = convert_tokenizer(tokenizer)\n",
    "    return connect_models(ov_tokenizer, ov_model)\n",
    "\n",
    "\n",
    "get_connected_model(hf_model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab371f7",
   "metadata": {},
   "source": [
    "Как видно из результатов на тестовом датасете:\n",
    "- Определённый дроп на валидационном датасете ничего не гарантирует.\n",
    "- Указать тип модели бывает важнее, чем дать валидационный датасет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "44fd4304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenVINO accuracy: 0.8956422018348624\n"
     ]
    }
   ],
   "source": [
    "ov.save_model(quntized_model_best, \"qbert_best.xml\", compress_to_fp16 = False)\n",
    "\n",
    "compiled_quantized_best = ov.compile_model(quntized_model_best)\n",
    "print(f\"OpenVINO accuracy: {accuracy_evaluate(compiled_quantized_best)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3669da21",
   "metadata": {},
   "source": [
    "156 FPS -> 285.53 FPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "23a6a2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ INFO ] Parsing input parameters\n",
      "[Step 2/11] Loading OpenVINO Runtime\n",
      "[ INFO ] OpenVINO:\n",
      "[ INFO ] Build ................................. 2025.0.0-17942-1f68be9f594-releases/2025/0\n",
      "[ INFO ] \n",
      "[ INFO ] Device info:\n",
      "[ INFO ] CPU\n",
      "[ INFO ] Build ................................. 2025.0.0-17942-1f68be9f594-releases/2025/0\n",
      "[ INFO ] \n",
      "[ INFO ] \n",
      "[Step 3/11] Setting device configuration\n",
      "[Step 4/11] Reading model files\n",
      "[ INFO ] Loading model files\n",
      "[ INFO ] Read model took 17.17 ms\n",
      "[ INFO ] Original model I/O parameters:\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input_ids (node: input_ids) : i64 / [...] / [?,?]\n",
      "[ INFO ]     attention_mask , 63 (node: attention_mask) : i64 / [...] / [?,?]\n",
      "[ INFO ]     token_type_ids (node: token_type_ids) : i64 / [...] / [?,?]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     logits (node: __module.classifier/aten::linear/Add) : f32 / [...] / [?,2]\n",
      "[Step 5/11] Resizing model to match image sizes and given batch\n",
      "[ INFO ] Model batch size: 1\n",
      "[ INFO ] Reshaping model: 'input_ids': [1,128], '63': [1,128], 'token_type_ids': [1,128]\n",
      "[ INFO ] Reshape model took 4.44 ms\n",
      "[Step 6/11] Configuring input of the model\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input_ids (node: input_ids) : i64 / [...] / [1,128]\n",
      "[ INFO ]     attention_mask , 63 (node: attention_mask) : i64 / [...] / [1,128]\n",
      "[ INFO ]     token_type_ids (node: token_type_ids) : i64 / [...] / [1,128]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     logits (node: __module.classifier/aten::linear/Add) : f32 / [...] / [1,2]\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Compile model took 408.40 ms\n",
      "[Step 8/11] Querying optimal runtime parameters\n",
      "[ INFO ] Model:\n",
      "[ INFO ]   NETWORK_NAME: Model54\n",
      "[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 12\n",
      "[ INFO ]   NUM_STREAMS: 12\n",
      "[ INFO ]   INFERENCE_NUM_THREADS: 16\n",
      "[ INFO ]   PERF_COUNT: NO\n",
      "[ INFO ]   INFERENCE_PRECISION_HINT: <Type: 'float32'>\n",
      "[ INFO ]   PERFORMANCE_HINT: THROUGHPUT\n",
      "[ INFO ]   EXECUTION_MODE_HINT: ExecutionMode.PERFORMANCE\n",
      "[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS: 0\n",
      "[ INFO ]   ENABLE_CPU_PINNING: False\n",
      "[ INFO ]   SCHEDULING_CORE_TYPE: SchedulingCoreType.ANY_CORE\n",
      "[ INFO ]   MODEL_DISTRIBUTION_POLICY: set()\n",
      "[ INFO ]   ENABLE_HYPER_THREADING: True\n",
      "[ INFO ]   EXECUTION_DEVICES: ['CPU']\n",
      "[ INFO ]   CPU_DENORMALS_OPTIMIZATION: False\n",
      "[ INFO ]   LOG_LEVEL: Level.NO\n",
      "[ INFO ]   CPU_SPARSE_WEIGHTS_DECOMPRESSION_RATE: 1.0\n",
      "[ INFO ]   DYNAMIC_QUANTIZATION_GROUP_SIZE: 32\n",
      "[ INFO ]   KV_CACHE_PRECISION: <Type: 'uint8_t'>\n",
      "[ INFO ]   KEY_CACHE_PRECISION: <Type: 'uint8_t'>\n",
      "[ INFO ]   VALUE_CACHE_PRECISION: <Type: 'uint8_t'>\n",
      "[ INFO ]   KEY_CACHE_GROUP_SIZE: 0\n",
      "[ INFO ]   VALUE_CACHE_GROUP_SIZE: 0\n",
      "[Step 9/11] Creating infer requests and preparing input tensors\n",
      "[ WARNING ] No input files were given for input 'input_ids'!. This input will be filled with random values!\n",
      "[ WARNING ] No input files were given for input '63'!. This input will be filled with random values!\n",
      "[ WARNING ] No input files were given for input 'token_type_ids'!. This input will be filled with random values!\n",
      "[ INFO ] Fill input 'input_ids' with random values \n",
      "[ INFO ] Fill input '63' with random values \n",
      "[ INFO ] Fill input 'token_type_ids' with random values \n",
      "[Step 10/11] Measuring performance (Start inference asynchronously, 12 inference requests, limits: 30000 ms duration)\n",
      "[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).\n",
      "[ INFO ] First inference took 18.05 ms\n",
      "[Step 11/11] Dumping statistics report\n",
      "[ INFO ] Execution Devices:['CPU']\n",
      "[ INFO ] Count:            8580 iterations\n",
      "[ INFO ] Duration:         30049.28 ms\n",
      "[ INFO ] Latency:\n",
      "[ INFO ]    Median:        38.61 ms\n",
      "[ INFO ]    Average:       41.91 ms\n",
      "[ INFO ]    Min:           23.98 ms\n",
      "[ INFO ]    Max:           205.59 ms\n",
      "[ INFO ] Throughput:   285.53 FPS\n"
     ]
    }
   ],
   "source": [
    "!benchmark_app -hint throughput -m qbert_best.xml -t 30 -shape \"input_ids[1,128],attention_mask[1,128],token_type_ids[1,128]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ae888f-f342-419b-8bb4-a9f226920821",
   "metadata": {},
   "source": [
    "# Дополнительно\n",
    "\n",
    "Добавим в модель:\n",
    "1. Токенизационный препроцессинг с помощью `openvino-tokenizers`\n",
    "2. Постпроцессинг в модель, чтобы она сразу отдавала результат `np.argmax(logits, axis=1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca177580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --pre -U openvino-tokenizers --extra-index-url https://storage.openvinotoolkit.org/simple/wheels/nightly git+https://github.com/openvinotoolkit/nncf.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027bb51d-32da-4526-a60c-d06a1f59b61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino_tokenizers import convert_tokenizer, connect_models\n",
    "\n",
    "\n",
    "def get_connected_model(hf_model, tokenizer):\n",
    "    example_input = {**tokenizer(\"test\", return_tensors=\"pt\")}\n",
    "    ov_model = ov.convert_model(hf_model, example_input=example_input)\n",
    "    ov_tokenizer = convert_tokenizer(tokenizer)\n",
    "    return connect_models(ov_tokenizer, ov_model)\n",
    "\n",
    "\n",
    "get_connected_model(hf_model, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
